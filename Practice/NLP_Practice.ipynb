{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Python String\n"
      ],
      "metadata": {
        "id": "5nQgOrhHZqaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P1HugxWZf4H",
        "outputId": "1ae30dbf-da21-4452-952f-a630246a44de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATa\n"
          ]
        }
      ],
      "source": [
        "a = \"DATa\"\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strings are arrays\n"
      ],
      "metadata": {
        "id": "VpGxNbitZ3J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the character at position 4 (Here, indexing starts from 0)\n",
        "a = \"DATASET\"\n",
        "print(a[4])  # Output: S\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5FBzlv9ZxGX",
        "outputId": "3b84d0e3-67a6-4c84-dee8-592739b9b5b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Slicing\n"
      ],
      "metadata": {
        "id": "2xa_vN4uZ8-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the output from the position 3 to 6(not included)\n",
        "a = \"Data test\"\n",
        "print(a[3:6])\n",
        "\n",
        "print(\"-\" * 50)  # This line prints 50 dashes\n",
        "\n",
        "# To get the output by negative indexing from 6 position to 2 position.\n",
        "print(a[-6:-2])\n",
        "\n",
        "print(\"-\" * 50)  # This line prints 50 dashes\n",
        "\n",
        "# Get the results from position 2 to 6 but give result with the increment of 2.\n",
        "print(a[2:6:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIkuZSHpaCNp",
        "outputId": "eda169b7-91f2-4932-8fd5-f70fa2e10494"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a t\n",
            "--------------------------------------------------\n",
            "a te\n",
            "--------------------------------------------------\n",
            "t \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###String Methods\n"
      ],
      "metadata": {
        "id": "nkKv4OY0aFqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lower() will lowercase the words which are upper in the sentences.\n",
        "a = \"data test\"\n",
        "print(a.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk1y2h3xaPsS",
        "outputId": "72a5a82e-0345-4a7c-b673-86c187fb13dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upper() will transform lowercase into upper.\n",
        "print(a.upper())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsdKl1L9aUA8",
        "outputId": "1bbc2ccb-5360-4b75-a6ed-977024f2ed9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA TEST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replace() will work like replace one string with another string.\n",
        "a = \"data test\"\n",
        "print(a.replace(\"i\", \"me\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKIuviWPaiDY",
        "outputId": "eaa4f0cc-1254-4f57-c330-85c496bed401"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split() will split the strings into substrings if it finds any instances of seprator.\n",
        "a = \"data test\"\n",
        "\n",
        "print(a.split(\"o\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suBfMKPJaksb",
        "outputId": "0399e3fe-ea8b-4632-b89e-a95486a6acb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###String Concatination"
      ],
      "metadata": {
        "id": "h6GcINxmanz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = \"Train\"\n",
        "a = \"data test\"\n",
        "\n",
        "print(a +\" \"+ b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC2JcSpxaudw",
        "outputId": "d4975f4b-0d54-42f8-89de-ae3e5cbe562f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data test Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###Create a text file\n"
      ],
      "metadata": {
        "id": "c4prXj7Kaz45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "file = open(\"datatest.txt\", \"w+\")\n",
        "\n",
        "# Here we declared file as a variable to open a file named iNeuron.txt. Open takes two arguemnets, first one is for the file we want to open and second\n",
        "# one represents some kind of permissions or operation we want to do into that file.\n",
        "\n",
        "# Here we taken \"W\" letter as an arguemnt, which indicates write and will create a file if it is not exist in the library.\n",
        "# That \"+\" signs indicate both read and write.\n",
        "\n",
        "# The other option beside \"w\" are, \"r\" for read, and \"a\" for append."
      ],
      "metadata": {
        "id": "dB98dMfga5FG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2\n",
        "for i in range(5):\n",
        "    file.write(\"Line number is %d\\r\\n\" % (i + 1))"
      ],
      "metadata": {
        "id": "n9v1MVONa-69"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "otlrl6ijbCCQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Append data to a file\n"
      ],
      "metadata": {
        "id": "YAJntVsIbEtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step1\n",
        "\n",
        "file= open(\"datatest.txt\", \"a+\")\n",
        "#Once again \"+\" sign is in the code which means if .txt file are not available,\n",
        "#this plus sign will create a new file but here is not any requirement to create a new file."
      ],
      "metadata": {
        "id": "Obt_5_gPbJMn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step2\n",
        "\n",
        "for i in range(3):\n",
        "    file.write(\"Appending Line number %d\\r\\n\" % (i+1))"
      ],
      "metadata": {
        "id": "Lf54J3T3bMve"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step3\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "aMON9DvdbfjJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read the files\n"
      ],
      "metadata": {
        "id": "y9ahdUVAboxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step1. Open the file in read mode\n",
        "\n",
        "file= open(\"datatest.txt\", \"r\")"
      ],
      "metadata": {
        "id": "eSa7efIAblmN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step2. Here we'll check, is our file is open or not, if yes we proceed\n",
        "if file.mode == 'r':\n",
        "    content=file.read()\n",
        "    #We used file.read() for reading the file data and store it in a variable."
      ],
      "metadata": {
        "id": "16wSFdn8cJYm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step3. Printing the file\n",
        "print(content)\n",
        "print(\"Here is the output!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsvKBm7BcOJK",
        "outputId": "086f1978-43af-4b1f-a324-a221c3f47e84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line number is 1\n",
            "Line number is 2\n",
            "Line number is 3\n",
            "Line number is 4\n",
            "Line number is 5\n",
            "Appending Line number 1\n",
            "Appending Line number 2\n",
            "Appending Line number 3\n",
            "\n",
            "Here is the output!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Preprocessing\n"
      ],
      "metadata": {
        "id": "xiHfH__8cSTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import nltk\n",
        "\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "el9tFzJxcXHJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Lowercase\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "input_str = \"Weather is too Cloudy. Possibility of Rain is High, Today!!\"\n",
        "lowercase_text(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QTWvSlVHca3g",
        "outputId": "7a7bb757-59f3-475d-9b4b-91bcba1f79f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weather is too cloudy. possibility of rain is high, today!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Removing numbers\n",
        "\n",
        "def remove_num(text):\n",
        "    result = re.sub(r'\\d+','', text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "gne-jjtzcgrX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_s =\"You bought 6 candies from shop, and 4 candies are in home.\"\n",
        "remove_num(input_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "txDCR3DAco5l",
        "outputId": "81e58533-5dd1-4e15-814b-92a8ee354f32"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You bought  candies from shop, and  candies are in home.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Owf0KmHcyZ6",
        "outputId": "066e45b1-de7b-41e4-bb4c-09f136bac219"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect) (2.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "\n",
        "# Initialize the inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Convert number into text\n",
        "def convert_num(text):\n",
        "    # Split strings into list of texts\n",
        "    temp_string = text.split()\n",
        "\n",
        "    # Initialize empty list\n",
        "    new_str = []\n",
        "\n",
        "    for word in temp_string:\n",
        "        # If text is a digit, convert the digit to words and append into the new str list\n",
        "        if word.isdigit():\n",
        "            new_str.append(p.number_to_words(word))\n",
        "        # Append the texts as it is\n",
        "        else:\n",
        "            new_str.append(word)\n",
        "\n",
        "    # Join the texts of new str to form a string\n",
        "    temp_str = ' '.join(new_str)\n",
        "\n",
        "    return temp_str\n",
        "\n",
        "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
        "print(convert_num(input_str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Z0P3GgdFIH",
        "outputId": "861a6bbd-fad8-4029-e9c3-a3c819778521"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bought six candies from shop, and four candies are in home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove punctuation.\n"
      ],
      "metadata": {
        "id": "MB5HIA3pdQXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's remove punctuation\n",
        "def rem_punct(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "5uoZSMrfdJmT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
        "rem_punct(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F8CfbKUSdXGg",
        "outputId": "55a847de-1c5b-42dc-93f1-5ad838e2ec97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey Are you excited After a week we will be in Shimla'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-78DTU9ddgRj",
        "outputId": "5f603535-46ef-480e-9687-4cb09c66c787"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove default stopwords\n"
      ],
      "metadata": {
        "id": "Iu1pXbdidkuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing nltk library\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYSyzqJZdikU",
        "outputId": "f268c76a-be2f-4a55-b3c1-23dd6034b94d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  \"\"\"Removes stopwords from a text string.\n",
        "\n",
        "  Args:\n",
        "    text: The text string to remove stopwords from.\n",
        "\n",
        "  Returns:\n",
        "    A new text string with stopwords removed.\n",
        "  \"\"\"\n",
        "\n",
        "  stop_words = set(stopwords.words(\"english\"))  # Import stopwords from nltk library\n",
        "  word_tokens = word_tokenize(text)  # Import word_tokenize from nltk library\n",
        "\n",
        "  # Filter out punctuation and stopwords\n",
        "  filtered_text = [word for word in word_tokens if word not in string.punctuation]\n",
        "  filtered_text = [word for word in filtered_text if word not in stop_words]\n",
        "\n",
        "  return filtered_text\n",
        "\n",
        "# Example usage\n",
        "text = \"Data is the new oil. A.I is the last invention\"\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23_GG22idt1g",
        "outputId": "05bca7d5-d04b-4c33-ace5-24aac06c3c8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data', 'new', 'oil', 'A.I', 'last', 'invention']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stemming\n"
      ],
      "metadata": {
        "id": "FDDqvV2qd4Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing nltk's porter stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#stem words in the list of tokenised words\n",
        "def stem_words(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stems = [stemmer.stem(word) for word in word_tokens]\n",
        "  return stems\n",
        "\n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYE7KhI7eAZB",
        "outputId": "9eedab73-174e-481b-ea79-713108ce8c61"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization\n"
      ],
      "metadata": {
        "id": "ERfE96CHeMBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def lemmatize_word(text):\n",
        "  \"\"\"Lemmatizes the text string.\n",
        "\n",
        "  Args:\n",
        "    text: The text string to lemmatize.\n",
        "\n",
        "  Returns:\n",
        "    A list of lemmatized words.\n",
        "  \"\"\"\n",
        "  # Download WordNet if not already downloaded\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "  # Tokenize the text\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  # Lemmatize each word\n",
        "  lemmas = [wordnet.WordNetLemmatizer().lemmatize(word, pos='v') for word in word_tokens]\n",
        "\n",
        "  return lemmas\n",
        "\n",
        "# Example usage\n",
        "text = \"Data is the new revolution in the World, in a day one individual would generate terabytes of data.\"\n",
        "lemmatized_words = lemmatize_word(text)\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJqjnAYweOkv",
        "outputId": "3c8db983-50df-479e-860b-6b2cab131008"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data', 'be', 'the', 'new', 'revolution', 'in', 'the', 'World', ',', 'in', 'a', 'day', 'one', 'individual', 'would', 'generate', 'terabytes', 'of', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parts of Speech (POS) Tagging\n"
      ],
      "metadata": {
        "id": "XhMo_bLQeU3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing tokenize library\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# download averaged perceptron tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# convert text into word tokens with their tags\n",
        "def pos_tagg(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return pos_tag(word_tokens)\n",
        "\n",
        "# Example usage\n",
        "pos_tagg(\"Are you afraid of something?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU3DiWy-eX3j",
        "outputId": "b94276b6-f834-446e-e87a-944184f17069"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the tagset\n",
        "nltk.download('tagsets')\n",
        "\n",
        "#extract information about the tag\n",
        "nltk.help.upenn_tagset('PRP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qctnon3Teg2x",
        "outputId": "dbfa645c-fd87-4a4a-8b36-15a7488e1252"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chunking\n"
      ],
      "metadata": {
        "id": "w8o5AgRWemUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# here we define chunking function with text and regular\n",
        "# expressions representing grammar as parameter\n",
        "def chunking(text, grammar):\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  # label words with pos\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "\n",
        "  # create chunk parser using grammar\n",
        "  chunkParser = nltk.RegexpParser(grammar)\n",
        "\n",
        "  # test it on the list of word tokens with tagged pos\n",
        "  tree = chunkParser.parse(word_pos)\n",
        "\n",
        "  # iterate over the parse tree and print subtrees\n",
        "  for subtree in tree.subtrees():\n",
        "    print(subtree)\n",
        "\n",
        "# sentence to be chunked\n",
        "sentence = 'the little red parrot is flying in the sky'\n",
        "\n",
        "# Regular expression grammar for Noun Phrase (NP)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Calling the chunking function with the sentence and grammar\n",
        "chunking(sentence, grammar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKPCtPMPeoOV",
        "outputId": "d01f4e92-5db7-48f4-f737-0e4a00483d58"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Named Entity Recognition\n"
      ],
      "metadata": {
        "id": "5aFdD03fesy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing tokenization and chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def ner(text):                    #tokenize the text\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  #pos tagging of words\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "\n",
        "  #tree of word entities\n",
        "  print(ne_chunk(word_pos))\n",
        "\n",
        "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
        "ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2RKN7xFevyU",
        "outputId": "b5729320-da9f-42e6-da3a-5c457e134c2b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  test/NN\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  played/VBD\n",
            "  in/IN\n",
            "  between/IN\n",
            "  (ORGANIZATION WI/NNP)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Understanding Regex\n",
        "As a software developer, you have probably encountered regular expressions many times and gotten confused many times with these daunting set of characters grouped together like this:\n",
        "\n",
        "Unsupported Cell Type. Double-Click to inspect/edit the content.\n",
        "And you may have wondered what this is all about?\n",
        "\n",
        "Regular Expressions (Regx or RegExp) are very useful in stepping up your algorithm game and this will make you a better problem solver. The structure of RegEx can be intimidating at first, but it is very rewarding once you got all the patterns and implemented them in your work properly.\n"
      ],
      "metadata": {
        "id": "51Hoy1Foez6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\", sent)\n",
        "\n",
        "print(r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q4Mqna-e8Hl",
        "outputId": "78134593-920f-4e1d-dca6-f9dab4794366"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example of expression in re.split function\n"
      ],
      "metadata": {
        "id": "l7MuUMvwe_6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"\\w+\", sent)  # Find all word characters at the beginning of the string\n",
        "\n",
        "print(r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-YAZQFZfBj7",
        "outputId": "715b7d9e-dda8-44e6-9171-805d45125122"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset', 'Data', 'is', 'a', 'new', 'fuel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example of is expression in re.split function\n"
      ],
      "metadata": {
        "id": "lIwS0H3FfF-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "print((re.split(r'\\s', 'We splited this sentence')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W1tkj8TfKxt",
        "outputId": "5a3fdbcb-f23e-46ba-c271-cd7aa26628e5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'splited', 'this', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "print((re.split(r's', 'We splited this sentence')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8WcDrCfQlz",
        "outputId": "d252b555-8cde-4098-fcd7-2afe3a483310"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We ', 'plited thi', ' ', 'entence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, there are series of regular expression in Python that you can use in various ways like \\d,\\D,$..,\\b, etc."
      ],
      "metadata": {
        "id": "gy7wviJqfWFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Use RegEx methods\n",
        "The \"re\" packages provide several methods to actually perform queries on an input string. We will see different methods which are\n",
        "\n",
        "                       re.match()\n",
        "\n",
        "                       re.search()\n",
        "\n",
        "                       re.findall()\n",
        "Note: Based on the RegEx, Python offers two different primitive operations. This match method checks for the match only at the beginning of the string while search checks for a match anywhere in the string.\n"
      ],
      "metadata": {
        "id": "DBqW-nMNfXJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using re.match()"
      ],
      "metadata": {
        "id": "4IDF7k4xhPky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "lists = ['icecream images', 'i immitated', 'inner peace']\n",
        "\n",
        "for i in lists:\n",
        "    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
        "\n",
        "    if q:\n",
        "        print(q.groups())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5d_RPWhW3c",
        "outputId": "68d25208-3c8f-4690-acd2-d22514b0c907"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('icecream', 'images')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Pattern in the text(re.search())\n"
      ],
      "metadata": {
        "id": "xe5rnELdhZ4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pattern = [\"playing\", \"dataset\"]\n",
        "text = \"Raju is playing outside.\"\n",
        "\n",
        "for p in pattern:\n",
        "    print(\"You're looking for '%s' in '%s'\" % (p, text), end='')\n",
        "\n",
        "    if re.search(p, text):\n",
        "        print('Found match!')\n",
        "    else:\n",
        "        print(\"no match found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-XHb3x8flcL",
        "outputId": "8bc06e3d-17a5-465b-9470-88c6c10c72c6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're looking for 'playing' in 'Raju is playing outside.'Found match!\n",
            "You're looking for 'dataset' in 'Raju is playing outside.'no match found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using re.findall() for text"
      ],
      "metadata": {
        "id": "WiIqWFUjhik5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "kgf = \"Abc@gmail.com, XYX@gmail.com, lmn@gmail.com, efg@gmail.com\"\n",
        "\n",
        "emails = re.findall(r' [\\w\\.]+@[\\w\\.]+', kgf)\n",
        "\n",
        "for e in emails:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys8Pr3hQhmL2",
        "outputId": "9d3af1f8-701f-4887-f199-26612bf661ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " XYX@gmail.com\n",
            " lmn@gmail.com\n",
            " efg@gmail.com\n"
          ]
        }
      ]
    }
  ]
}